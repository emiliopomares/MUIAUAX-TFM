{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14efa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "# \n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "from logging import getLogger\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch._C import _set_backcompat_keepdim_warn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "N_MAX_POSITIONS = 4096  # maximum input sequence length\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def Embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
    "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n",
    "    if padding_idx is not None:\n",
    "        nn.init.constant_(m.weight[padding_idx], 0)\n",
    "    return m\n",
    "\n",
    "\n",
    "def create_sinusoidal_embeddings(n_pos, dim, out):\n",
    "    position_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
    "            for pos in range(n_pos)\n",
    "        ]\n",
    "    )\n",
    "    out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "    out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "    out.detach_()\n",
    "    out.requires_grad = False\n",
    "\n",
    "\n",
    "def get_masks(slen, lengths, causal):\n",
    "    \"\"\"\n",
    "    Generate hidden states mask, and optionally an attention mask.\n",
    "    \"\"\"\n",
    "    assert lengths.max().item() <= slen\n",
    "    bs = lengths.size(0)\n",
    "    alen = torch.arange(slen, dtype=torch.long, device=lengths.device)\n",
    "    mask = alen < lengths[:, None]\n",
    "\n",
    "    # attention mask is the same as mask, or triangular inferior attention (causal)\n",
    "    if causal:\n",
    "        attn_mask = alen[None, None, :].repeat(bs, slen, 1) <= alen[None, :, None]\n",
    "    else:\n",
    "        attn_mask = mask\n",
    "\n",
    "    # sanity check\n",
    "    assert mask.size() == (bs, slen)\n",
    "    assert causal is False or attn_mask.size() == (bs, slen, slen)\n",
    "\n",
    "    return mask, attn_mask\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    NEW_ID = itertools.count()\n",
    "\n",
    "    def __init__(self, n_heads, dim, src_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layer_id = next(MultiHeadAttention.NEW_ID)\n",
    "        self.dim = dim\n",
    "        self.src_dim = src_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        assert self.dim % self.n_heads == 0\n",
    "\n",
    "        self.q_lin = nn.Linear(dim, dim)\n",
    "        self.k_lin = nn.Linear(src_dim, dim)\n",
    "        self.v_lin = nn.Linear(src_dim, dim)\n",
    "        self.out_lin = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, input, mask, kv=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        Self-attention (if kv is None)\n",
    "        or attention over source sentence (provided by kv).\n",
    "        Input is (bs, qlen, dim)\n",
    "        Mask is (bs, klen) (non-causal) or (bs, klen, klen)\n",
    "        \"\"\"\n",
    "        assert not (use_cache and self.cache is None)\n",
    "        bs, qlen, dim = input.size()\n",
    "        if kv is None:\n",
    "            klen = qlen if not use_cache else self.cache[\"slen\"] + qlen\n",
    "        else:\n",
    "            klen = kv.size(1)\n",
    "        assert dim == self.dim, \"Dimensions do not match: %s input vs %s configured\" % (\n",
    "            dim,\n",
    "            self.dim,\n",
    "        )\n",
    "        n_heads = self.n_heads\n",
    "        dim_per_head = dim // n_heads\n",
    "        mask_reshape = (bs, 1, qlen, klen) if mask.dim() == 3 else (bs, 1, 1, klen)\n",
    "\n",
    "        def shape(x):\n",
    "            \"\"\"  projection \"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "\n",
    "        def unshape(x):\n",
    "            \"\"\"  compute context \"\"\"\n",
    "            return (\n",
    "                x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "            )\n",
    "\n",
    "        q = shape(self.q_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n",
    "        if kv is None:\n",
    "            k = shape(self.k_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n",
    "            v = shape(self.v_lin(input))  # (bs, n_heads, qlen, dim_per_head)\n",
    "        elif not use_cache or self.layer_id not in self.cache:\n",
    "            k = v = kv\n",
    "            k = shape(self.k_lin(k))  # (bs, n_heads, qlen, dim_per_head)\n",
    "            v = shape(self.v_lin(v))  # (bs, n_heads, qlen, dim_per_head)\n",
    "\n",
    "        if use_cache:\n",
    "            if self.layer_id in self.cache:\n",
    "                if kv is None:\n",
    "                    k_, v_ = self.cache[self.layer_id]\n",
    "                    k = torch.cat([k_, k], dim=2)  # (bs, n_heads, klen, dim_per_head)\n",
    "                    v = torch.cat([v_, v], dim=2)  # (bs, n_heads, klen, dim_per_head)\n",
    "                else:\n",
    "                    k, v = self.cache[self.layer_id]\n",
    "            self.cache[self.layer_id] = (k, v)\n",
    "        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, qlen, dim_per_head)\n",
    "        scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, qlen, klen)\n",
    "        mask = (\n",
    "            (mask == 0).view(mask_reshape).expand_as(scores)\n",
    "        )  # (bs, n_heads, qlen, klen)\n",
    "        scores.masked_fill_(mask, -float(\"inf\"))  # (bs, n_heads, qlen, klen)\n",
    "\n",
    "        weights = F.softmax(scores.float(), dim=-1).type_as(\n",
    "            scores\n",
    "        )  # (bs, n_heads, qlen, klen)\n",
    "        weights = F.dropout(\n",
    "            weights, p=self.dropout, training=self.training\n",
    "        )  # (bs, n_heads, qlen, klen)\n",
    "        context = torch.matmul(weights, v)  # (bs, n_heads, qlen, dim_per_head)\n",
    "        context = unshape(context)  # (bs, qlen, dim)\n",
    "\n",
    "        if TransformerModel.STORE_OUTPUTS and not self.training:\n",
    "            self.outputs = weights.detach().cpu()\n",
    "\n",
    "        return self.out_lin(context)\n",
    "\n",
    "\n",
    "class TransformerFFN(nn.Module):\n",
    "    def __init__(self, in_dim, dim_hidden, out_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.act = F.relu\n",
    "        self.lin1 = nn.Linear(in_dim, dim_hidden)\n",
    "        self.lin2 = nn.Linear(dim_hidden, out_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.lin1(input)\n",
    "        x = self.act(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, params, is_encoder):\n",
    "        \"\"\"\n",
    "        Transformer model (encoder or decoder).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_encoder = is_encoder\n",
    "        self.is_decoder = not is_encoder\n",
    "\n",
    "        # model parameters\n",
    "        self.dim = params.enc_emb_dim if is_encoder else params.dec_emb_dim  # 512 by default\n",
    "        self.src_dim = params.enc_emb_dim\n",
    "        self.hidden_dim = self.dim * 4  # 2048 by default\n",
    "        self.n_heads = params.n_enc_heads if is_encoder else params.n_dec_heads  # 8 by default\n",
    "        self.n_layers = params.n_enc_layers if is_encoder else params.n_dec_layers\n",
    "        self.dropout = params.dropout\n",
    "        self.attention_dropout = params.attention_dropout\n",
    "        \n",
    "        assert (\n",
    "            self.dim % self.n_heads == 0\n",
    "        ), \"transformer dim must be a multiple of n_heads\"\n",
    "        self.self_attention = MultiHeadAttention(\n",
    "            self.n_heads,\n",
    "            self.dim,\n",
    "            self.dim,\n",
    "            dropout=self.attention_dropout,\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(self.dim, eps=1e-12)\n",
    "        if self.is_decoder:\n",
    "            self.layer_norm15 = nn.LayerNorm(self.dim, eps=1e-12)\n",
    "            self.cross_attention = MultiHeadAttention(\n",
    "                self.n_heads,\n",
    "                self.dim,\n",
    "                self.src_dim,\n",
    "                dropout=self.attention_dropout,\n",
    "            )\n",
    "        self.ffn = TransformerFFN(\n",
    "            self.dim,\n",
    "            self.hidden_dim,\n",
    "            self.dim,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(self.dim, eps=1e-12)\n",
    "\n",
    "    def forward(self, x, attn_mask, src_mask, src_enc, use_cache=False, cache=None):\n",
    "        tensor = x\n",
    "        # self attention\n",
    "        self.self_attention.cache = cache\n",
    "        attn = self.self_attention(tensor, attn_mask, use_cache=use_cache)\n",
    "        attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "        output = tensor + attn\n",
    "        output = self.layer_norm1(output)\n",
    "\n",
    "        # encoder attention (for decoder only)\n",
    "        if self.is_decoder and src_enc is not None:\n",
    "            self.cross_attention.cache = cache\n",
    "            attn = self.cross_attention(tensor, src_mask, kv=src_enc, use_cache=use_cache)\n",
    "            attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "            output = output + attn\n",
    "            output = self.layer_norm15(output)\n",
    "\n",
    "        # FFN\n",
    "        output = output + self.ffn(output)\n",
    "        tensor = self.layer_norm2(output)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    STORE_OUTPUTS = False\n",
    "\n",
    "    def __init__(self, params, id2word, is_encoder, with_output):\n",
    "        \"\"\"\n",
    "        Transformer model (encoder or decoder).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder / decoder, output layer\n",
    "        self.dtype = torch.half if params.fp16 else torch.float\n",
    "        self.is_encoder = is_encoder\n",
    "        self.is_decoder = not is_encoder\n",
    "        self.with_output = with_output\n",
    "\n",
    "        # dictionary\n",
    "        self.n_words = params.n_words\n",
    "        self.eos_index = params.eos_index\n",
    "        self.pad_index = params.pad_index\n",
    "        self.id2word = id2word\n",
    "        assert len(self.id2word) == self.n_words\n",
    "\n",
    "        # model parameters\n",
    "        self.dim = params.enc_emb_dim if is_encoder else params.dec_emb_dim  # 512 by default\n",
    "        self.src_dim = params.enc_emb_dim\n",
    "        self.hidden_dim = self.dim * 4  # 2048 by default\n",
    "        self.n_heads = params.n_enc_heads if is_encoder else params.n_dec_heads  # 8 by default\n",
    "        self.n_layers = params.n_enc_layers if is_encoder else params.n_dec_layers\n",
    "        self.dropout = params.dropout\n",
    "        self.attention_dropout = params.attention_dropout\n",
    "        assert (\n",
    "            self.dim % self.n_heads == 0\n",
    "        ), \"transformer dim must be a multiple of n_heads\"\n",
    "        \n",
    "        # embeddings\n",
    "        self.position_embeddings = Embedding(N_MAX_POSITIONS, self.dim)\n",
    "        if params.sinusoidal_embeddings:\n",
    "            create_sinusoidal_embeddings(\n",
    "                N_MAX_POSITIONS, self.dim, out=self.position_embeddings.weight\n",
    "            )\n",
    "        self.embeddings = Embedding(self.n_words, self.dim, padding_idx=self.pad_index)\n",
    "        self.layer_norm_emb = nn.LayerNorm(self.dim, eps=1e-12)\n",
    "\n",
    "        # transformer layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_id in range(self.n_layers):\n",
    "            self.layers.append(TransformerLayer(params, self.is_encoder))\n",
    "\n",
    "        self.cache = None\n",
    "\n",
    "        # output layer\n",
    "        if self.with_output:\n",
    "            self.proj = nn.Linear(self.dim, params.n_words, bias=True)\n",
    "            if params.share_inout_emb:\n",
    "                self.proj.weight = self.embeddings.weight\n",
    "\n",
    "    def forward(self, mode, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward function with different forward modes.\n",
    "        ### Small hack to handle PyTorch distributed.\n",
    "        \"\"\"\n",
    "        if mode == \"fwd\":\n",
    "            return self.fwd(**kwargs)\n",
    "        elif mode == \"predict\":\n",
    "            return self.predict(**kwargs)\n",
    "        else:\n",
    "            raise Exception(\"Unknown mode: %s\" % mode)\n",
    "\n",
    "    def fwd(\n",
    "        self,\n",
    "        x,\n",
    "        lengths,\n",
    "        causal,\n",
    "        src_enc=None,\n",
    "        src_len=None,\n",
    "        positions=None,\n",
    "        use_cache=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            `x` LongTensor(slen, bs), containing word indices\n",
    "            `lengths` LongTensor(bs), containing the length of each sentence\n",
    "            `causal` Boolean, if True, the attention is only done over previous hidden states\n",
    "            `positions` LongTensor(slen, bs), containing word positions\n",
    "        \"\"\"\n",
    "        # lengths = (x != self.pad_index).float().sum(dim=1)\n",
    "        # mask = x != self.pad_index\n",
    "\n",
    "        # check inputs\n",
    "        slen, bs = x.size()\n",
    "        assert lengths.size(0) == bs\n",
    "        assert lengths.max().item() <= slen\n",
    "        x = x.transpose(0, 1)  # batch size as dimension 0\n",
    "        assert (src_enc is None) == (src_len is None)\n",
    "        if src_enc is not None:\n",
    "            assert self.is_decoder\n",
    "            assert src_enc.size(0) == bs\n",
    "        assert not (use_cache and self.cache is None)\n",
    "\n",
    "        # generate masks\n",
    "        mask, attn_mask = get_masks(slen, lengths, causal)\n",
    "        src_mask = None\n",
    "        if self.is_decoder and src_enc is not None:\n",
    "            src_mask = (\n",
    "                torch.arange(src_len.max(), dtype=torch.long, device=lengths.device)\n",
    "                < src_len[:, None]\n",
    "            )\n",
    "        \n",
    "        # positions\n",
    "        if positions is None:\n",
    "            positions = x.new(slen).long()\n",
    "            positions = torch.arange(slen, out=positions).unsqueeze(0)\n",
    "        else:\n",
    "            assert positions.size() == (slen, bs)\n",
    "            positions = positions.transpose(0, 1)\n",
    "\n",
    "        # do not recompute cached elements\n",
    "        if use_cache:\n",
    "            _slen = slen - self.cache[\"slen\"]\n",
    "            x = x[:, -_slen:]\n",
    "            positions = positions[:, -_slen:]\n",
    "            mask = mask[:, -_slen:]\n",
    "            attn_mask = attn_mask[:, -_slen:]\n",
    "\n",
    "        # all layer outputs\n",
    "        if TransformerModel.STORE_OUTPUTS and not self.training:\n",
    "            self.outputs = []\n",
    "\n",
    "        # embeddings\n",
    "        tensor = self.embeddings(x)\n",
    "        tensor = tensor + self.position_embeddings(positions).expand_as(tensor)\n",
    "        tensor = self.layer_norm_emb(tensor)\n",
    "        tensor = F.dropout(tensor, p=self.dropout, training=self.training)\n",
    "        tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
    "        if TransformerModel.STORE_OUTPUTS and not self.training:\n",
    "            self.outputs.append(tensor.detach().cpu())\n",
    "\n",
    "        # transformer layers\n",
    "        for i in range(self.n_layers):\n",
    "            tensor = self.layers[i].forward(tensor, attn_mask, src_mask, src_enc, use_cache=use_cache, cache=self.cache)\n",
    "    \n",
    "            tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
    "            if TransformerModel.STORE_OUTPUTS and not self.training:\n",
    "                self.outputs.append(tensor.detach().cpu())\n",
    "\n",
    "        # update cache length\n",
    "        if use_cache:\n",
    "            self.cache[\"slen\"] += tensor.size(1)\n",
    "\n",
    "        # move back sequence length to dimension 0\n",
    "        tensor = tensor.transpose(0, 1)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def predict(self, tensor, pred_mask, y, get_scores):\n",
    "        \"\"\"\n",
    "        Given the last hidden state, compute word scores and/or the loss.\n",
    "            `pred_mask` is a ByteTensor of shape (slen, bs), filled with 1 when\n",
    "                we need to predict a word\n",
    "            `y` is a LongTensor of shape (pred_mask.sum(),)\n",
    "            `get_scores` is a boolean specifying whether we need to return scores\n",
    "        \"\"\"\n",
    "        x = tensor[pred_mask.unsqueeze(-1).expand_as(tensor)].view(-1, self.dim)\n",
    "        assert (y == self.pad_index).sum().item() == 0\n",
    "        scores = self.proj(x).view(-1, self.n_words)\n",
    "        loss = F.cross_entropy(scores.float(), y, reduction=\"mean\")\n",
    "        return scores, loss\n",
    "\n",
    "    def generate(self, src_enc, src_len, max_len=200, sample_temperature=None):\n",
    "        \"\"\"\n",
    "        Decode a sentence given initial start.\n",
    "        `x`:\n",
    "            - LongTensor(bs, slen)\n",
    "                <EOS> W1 W2 W3 <EOS> <PAD>\n",
    "                <EOS> W1 W2 W3   W4  <EOS>\n",
    "        `lengths`:\n",
    "            - LongTensor(bs) [5, 6]\n",
    "        `positions`:\n",
    "            - False, for regular \"arange\" positions (LM)\n",
    "            - True, to reset positions from the new generation (MT)\n",
    "        \"\"\"\n",
    "\n",
    "        # input batch\n",
    "        bs = len(src_len)\n",
    "        assert src_enc.size(0) == bs\n",
    "\n",
    "        # generated sentences\n",
    "        generated = src_len.new(max_len, bs)  # upcoming output\n",
    "        generated.fill_(self.pad_index)  # fill upcoming ouput with <PAD>\n",
    "        generated[0].fill_(self.eos_index)  # we use <EOS> for <BOS> everywhere\n",
    "\n",
    "        # positions\n",
    "        positions = src_len.new(max_len).long()\n",
    "        positions = (\n",
    "            torch.arange(max_len, out=positions).unsqueeze(1).expand_as(generated)\n",
    "        )\n",
    "\n",
    "        # current position / max lengths / length of generated sentences / unfinished sentences\n",
    "        cur_len = 1\n",
    "        gen_len = src_len.clone().fill_(1)\n",
    "        unfinished_sents = src_len.clone().fill_(1)\n",
    "\n",
    "        # cache compute states\n",
    "        self.cache = {\"slen\": 0}\n",
    "\n",
    "        while cur_len < max_len:\n",
    "\n",
    "            # compute word scores\n",
    "            tensor = self.forward(\n",
    "                \"fwd\",\n",
    "                x=generated[:cur_len],\n",
    "                lengths=gen_len,\n",
    "                positions=positions[:cur_len],\n",
    "                causal=True,\n",
    "                src_enc=src_enc,\n",
    "                src_len=src_len,\n",
    "                use_cache=True,\n",
    "            )\n",
    "            assert tensor.size() == (1, bs, self.dim)\n",
    "            tensor = tensor.data[-1, :, :] # .to(self.dtype)  # (bs, dim)\n",
    "            scores = self.proj(tensor)  # (bs, n_words)\n",
    "\n",
    "            # select next words: sample or greedy\n",
    "            if sample_temperature is None:\n",
    "                next_words = torch.topk(scores, 1)[1].squeeze(1)\n",
    "            else:\n",
    "                next_words = torch.multinomial(\n",
    "                    F.softmax(scores.float() / sample_temperature, dim=1), 1\n",
    "                ).squeeze(1)\n",
    "            assert next_words.size() == (bs,)\n",
    "\n",
    "            # update generations / lengths / finished sentences / current length\n",
    "            generated[cur_len] = next_words * unfinished_sents + self.pad_index * (\n",
    "                1 - unfinished_sents\n",
    "            )\n",
    "            gen_len.add_(unfinished_sents)\n",
    "            unfinished_sents.mul_(next_words.ne(self.eos_index).long())\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            # stop when there is a </s> in each sentence, or if we exceed the maximal length\n",
    "            if unfinished_sents.max() == 0:\n",
    "                break\n",
    "\n",
    "        # add <EOS> to unfinished sentences\n",
    "        if cur_len == max_len:\n",
    "            generated[-1].masked_fill_(unfinished_sents.byte(), self.eos_index)\n",
    "\n",
    "        # sanity check\n",
    "        assert (generated == self.eos_index).sum() == 2 * bs\n",
    "\n",
    "        return generated[:cur_len], gen_len\n",
    "\n",
    "    def generate_beam(\n",
    "        self, src_enc, src_len, beam_size, length_penalty, early_stopping, max_len=200\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Decode a sentence given initial start.\n",
    "        `x`:\n",
    "            - LongTensor(bs, slen)\n",
    "                <EOS> W1 W2 W3 <EOS> <PAD>\n",
    "                <EOS> W1 W2 W3   W4  <EOS>\n",
    "        `lengths`:\n",
    "            - LongTensor(bs) [5, 6]\n",
    "        `positions`:\n",
    "            - False, for regular \"arange\" positions (LM)\n",
    "            - True, to reset positions from the new generation (MT)\n",
    "        \"\"\"\n",
    "\n",
    "        # check inputs\n",
    "        assert src_enc.size(0) == src_len.size(0)\n",
    "        assert beam_size >= 1\n",
    "\n",
    "        # batch size / number of words\n",
    "        bs = len(src_len)\n",
    "        n_words = self.n_words\n",
    "\n",
    "        # expand to beam size the source latent representations / source lengths\n",
    "        src_enc = (\n",
    "            src_enc.unsqueeze(1)\n",
    "            .expand((bs, beam_size) + src_enc.shape[1:])\n",
    "            .contiguous()\n",
    "            .view((bs * beam_size,) + src_enc.shape[1:])\n",
    "        )\n",
    "        src_len = src_len.unsqueeze(1).expand(bs, beam_size).contiguous().view(-1)\n",
    "\n",
    "        # generated sentences (batch with beam current hypotheses)\n",
    "        generated = src_len.new(max_len, bs * beam_size)  # upcoming output\n",
    "        generated.fill_(self.pad_index)  # fill upcoming ouput with <PAD>\n",
    "        generated[0].fill_(self.eos_index)  # we use <EOS> for <BOS> everywhere\n",
    "\n",
    "        # generated hypotheses\n",
    "        generated_hyps = [\n",
    "            BeamHypotheses(beam_size, max_len, length_penalty, early_stopping)\n",
    "            for _ in range(bs)\n",
    "        ]\n",
    "\n",
    "        # positions\n",
    "        positions = src_len.new(max_len).long()\n",
    "        positions = (\n",
    "            torch.arange(max_len, out=positions).unsqueeze(1).expand_as(generated)\n",
    "        )\n",
    "\n",
    "        # scores for each sentence in the beam\n",
    "        beam_scores = src_enc.new(bs, beam_size).float().fill_(0)\n",
    "        beam_scores[:, 1:] = -1e9\n",
    "        beam_scores = beam_scores.view(-1)\n",
    "\n",
    "        # current position\n",
    "        cur_len = 1\n",
    "\n",
    "        # cache compute states\n",
    "        self.cache = {\"slen\": 0}\n",
    "\n",
    "        # done sentences\n",
    "        done = [False for _ in range(bs)]\n",
    "\n",
    "        while cur_len < max_len:\n",
    "\n",
    "            # compute word scores\n",
    "            tensor = self.forward(\n",
    "                \"fwd\",\n",
    "                x=generated[:cur_len],\n",
    "                lengths=src_len.new(bs * beam_size).fill_(cur_len),\n",
    "                positions=positions[:cur_len],\n",
    "                causal=True,\n",
    "                src_enc=src_enc,\n",
    "                src_len=src_len,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "            assert tensor.size() == (1, bs * beam_size, self.dim)\n",
    "            tensor = tensor.data[-1, :, :]  # .to(self.dtype)  # (bs * beam_size, dim)\n",
    "            scores = self.proj(tensor)  # (bs * beam_size, n_words)\n",
    "            scores = F.log_softmax(scores.float(), dim=-1)  # (bs * beam_size, n_words)\n",
    "            assert scores.size() == (bs * beam_size, n_words)\n",
    "\n",
    "            # select next words with scores\n",
    "            _scores = scores + beam_scores[:, None].expand_as(\n",
    "                scores\n",
    "            )  # (bs * beam_size, n_words)\n",
    "            _scores = _scores.view(bs, beam_size * n_words)  # (bs, beam_size * n_words)\n",
    "\n",
    "            next_scores, next_words = torch.topk(\n",
    "                _scores, 2 * beam_size, dim=1, largest=True, sorted=True\n",
    "            )\n",
    "            assert next_scores.size() == next_words.size() == (bs, 2 * beam_size)\n",
    "\n",
    "            # next batch beam content\n",
    "            # list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)\n",
    "            next_batch_beam = []\n",
    "\n",
    "            # for each sentence\n",
    "            for sent_id in range(bs):\n",
    "\n",
    "                # if we are done with this sentence\n",
    "                done[sent_id] = done[sent_id] or generated_hyps[sent_id].is_done(\n",
    "                    next_scores[sent_id].max().item()\n",
    "                )\n",
    "                if done[sent_id]:\n",
    "                    next_batch_beam.extend(\n",
    "                        [(0, self.pad_index, 0)] * beam_size\n",
    "                    )  # pad the batch\n",
    "                    continue\n",
    "\n",
    "                # next sentence beam content\n",
    "                next_sent_beam = []\n",
    "\n",
    "                # next words for this sentence\n",
    "                for idx, value in zip(next_words[sent_id], next_scores[sent_id]):\n",
    "\n",
    "                    # get beam and word IDs\n",
    "                    beam_id = idx // n_words\n",
    "                    word_id = idx % n_words\n",
    "\n",
    "                    # end of sentence, or next word\n",
    "                    if word_id == self.eos_index or cur_len + 1 == max_len:\n",
    "                        generated_hyps[sent_id].add(\n",
    "                            generated[:cur_len, sent_id * beam_size + beam_id]\n",
    "                            .clone()\n",
    "                            .cpu(),\n",
    "                            value.item(),\n",
    "                        )\n",
    "                    else:\n",
    "                        next_sent_beam.append(\n",
    "                            (value, word_id, sent_id * beam_size + beam_id)\n",
    "                        )\n",
    "\n",
    "                    # the beam for next step is full\n",
    "                    if len(next_sent_beam) == beam_size:\n",
    "                        break\n",
    "\n",
    "                # update next beam content\n",
    "                assert len(next_sent_beam) == 0 if cur_len + 1 == max_len else beam_size\n",
    "                if len(next_sent_beam) == 0:\n",
    "                    next_sent_beam = [\n",
    "                        (0, self.pad_index, 0)\n",
    "                    ] * beam_size  # pad the batch\n",
    "                next_batch_beam.extend(next_sent_beam)\n",
    "                assert len(next_batch_beam) == beam_size * (sent_id + 1)\n",
    "\n",
    "            # sanity check / prepare next batch\n",
    "            assert len(next_batch_beam) == bs * beam_size\n",
    "            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
    "            beam_words = generated.new([x[1] for x in next_batch_beam])\n",
    "            beam_idx = src_len.new([x[2] for x in next_batch_beam])\n",
    "\n",
    "            # re-order batch and internal states\n",
    "            generated = generated[:, beam_idx]\n",
    "            generated[cur_len] = beam_words\n",
    "            for k in self.cache.keys():\n",
    "                if k != \"slen\":\n",
    "                    self.cache[k] = (\n",
    "                        self.cache[k][0][beam_idx],\n",
    "                        self.cache[k][1][beam_idx],\n",
    "                    )\n",
    "\n",
    "            # update current length\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            # stop when we are done with each sentence\n",
    "            if all(done):\n",
    "                break\n",
    "\n",
    "        # def get_coeffs(s):\n",
    "        #     roots = [int(s[i + 2]) for i, c in enumerate(s) if c == 'x']\n",
    "        #     poly = np.poly1d(roots, r=True)\n",
    "        #     coeffs = list(poly.coefficients.astype(np.int64))\n",
    "        #     return [c % 10 for c in coeffs], coeffs\n",
    "\n",
    "        # visualize hypotheses\n",
    "        # print([len(x) for x in generated_hyps], cur_len)\n",
    "        # globals().update( locals() );\n",
    "        # !import code; code.interact(local=vars())\n",
    "        # for ii in range(bs):\n",
    "        #     for ss, ww in sorted(generated_hyps[ii].hyp, key=lambda x: x[0], reverse=True):\n",
    "        #         hh = \" \".join(self.id2word[x] for x in ww.tolist())\n",
    "        #         print(f\"{ss:+.4f} {hh}\")\n",
    "        #         # cc = get_coeffs(hh[4:])\n",
    "        #         # print(f\"{ss:+.4f} {hh} || {cc[0]} || {cc[1]}\")\n",
    "        #     print(\"\")\n",
    "\n",
    "        # select the best hypotheses\n",
    "        tgt_len = src_len.new(bs)\n",
    "        best = []\n",
    "\n",
    "        for i, hypotheses in enumerate(generated_hyps):\n",
    "            best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n",
    "            tgt_len[i] = len(best_hyp) + 1  # +1 for the <EOS> symbol\n",
    "            best.append(best_hyp)\n",
    "\n",
    "        # generate target batch\n",
    "        decoded = src_len.new(tgt_len.max().item(), bs).fill_(self.pad_index)\n",
    "        for i, hypo in enumerate(best):\n",
    "            decoded[: tgt_len[i] - 1, i] = hypo\n",
    "            decoded[tgt_len[i] - 1, i] = self.eos_index\n",
    "\n",
    "        # sanity check\n",
    "        assert (decoded == self.eos_index).sum() == 2 * bs\n",
    "\n",
    "        return decoded, tgt_len, generated_hyps\n",
    "\n",
    "\n",
    "class BeamHypotheses(object):\n",
    "    def __init__(self, n_hyp, max_len, length_penalty, early_stopping):\n",
    "        \"\"\"\n",
    "        Initialize n-best list of hypotheses.\n",
    "        \"\"\"\n",
    "        self.max_len = max_len - 1  # ignoring <BOS>\n",
    "        self.length_penalty = length_penalty\n",
    "        self.early_stopping = early_stopping\n",
    "        self.n_hyp = n_hyp\n",
    "        self.hyp = []\n",
    "        self.worst_score = 1e9\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses in the list.\n",
    "        \"\"\"\n",
    "        return len(self.hyp)\n",
    "\n",
    "    def add(self, hyp, sum_logprobs):\n",
    "        \"\"\"\n",
    "        Add a new hypothesis to the list.\n",
    "        \"\"\"\n",
    "        score = sum_logprobs / len(hyp) ** self.length_penalty\n",
    "        if len(self) < self.n_hyp or score > self.worst_score:\n",
    "            self.hyp.append((score, hyp))\n",
    "            if len(self) > self.n_hyp:\n",
    "                sorted_scores = sorted(\n",
    "                    [(s, idx) for idx, (s, _) in enumerate(self.hyp)]\n",
    "                )\n",
    "                del self.hyp[sorted_scores[0][1]]\n",
    "                self.worst_score = sorted_scores[1][0]\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score)\n",
    "\n",
    "    def is_done(self, best_sum_logprobs):\n",
    "        \"\"\"\n",
    "        If there are enough hypotheses and that none of the hypotheses being generated\n",
    "        can become better than the worst one in the heap,\n",
    "        then we are done with this sentence.\n",
    "        \"\"\"\n",
    "        if len(self) < self.n_hyp:\n",
    "            return False\n",
    "        elif self.early_stopping:\n",
    "            return True\n",
    "        else:\n",
    "            return (\n",
    "                self.worst_score\n",
    "                >= best_sum_logprobs / self.max_len ** self.length_penalty\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "112b5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe15f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import struct\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21826c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_box = (80, 0, 1280, 720)\n",
    "l_img = Image.open(f\"1L.png\").convert(\"RGB\").crop(l_box).resize((224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a400ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(l_img)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b8986c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches: 49\n",
      "Shape of patches: (49, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def patchify_image(image, patch_size):\n",
    "    \"\"\"\n",
    "    Function to patchify an input image.\n",
    "\n",
    "    Args:\n",
    "    - image_path (str): Path to the input image.\n",
    "    - patch_size (int): Size of each patch.\n",
    "\n",
    "    Returns:\n",
    "    - patches (np.array): Patchified image in the shape of (num_patches, patch_size, patch_size, channels).\n",
    "    - num_patches (int): Number of patches.\n",
    "    \"\"\"\n",
    "    img_height, img_width, _ = image.shape\n",
    "\n",
    "    # Calculate number of patches in height and width\n",
    "    num_patches_h = img_height // patch_size\n",
    "    num_patches_w = img_width // patch_size\n",
    "\n",
    "    # Crop the image to fit an integer number of patches\n",
    "    image = image[:num_patches_h * patch_size, :num_patches_w * patch_size, :]\n",
    "\n",
    "    # Reshape image into patches\n",
    "    patches = np.reshape(image, (num_patches_h, patch_size, num_patches_w, patch_size, -1))\n",
    "    patches = np.transpose(patches, (0, 2, 1, 3, 4))\n",
    "    patches = np.reshape(patches, (-1, patch_size, patch_size, img.shape[2]))\n",
    "\n",
    "    return patches, num_patches_h * num_patches_w\n",
    "\n",
    "# Example usage:\n",
    "#image_path = 'example_image.jpg'\n",
    "#patch_size = 32  # Choose appropriate patch size\n",
    "patches, num_patches = patchify_image(img, patch_size)\n",
    "print(\"Number of patches:\", num_patches)\n",
    "print(\"Shape of patches:\", patches.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0170cac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.16470588, 0.15294118, 0.16862745],\n",
       "        [0.17647059, 0.16470588, 0.18039216],\n",
       "        [0.16862745, 0.16078431, 0.17254902],\n",
       "        ...,\n",
       "        [0.20784314, 0.19215686, 0.20392157],\n",
       "        [0.21568627, 0.2       , 0.21568627],\n",
       "        [0.21960784, 0.20784314, 0.21960784]],\n",
       "\n",
       "       [[0.15686275, 0.14901961, 0.16470588],\n",
       "        [0.16078431, 0.14901961, 0.16470588],\n",
       "        [0.16470588, 0.15294118, 0.16862745],\n",
       "        ...,\n",
       "        [0.21568627, 0.20392157, 0.21568627],\n",
       "        [0.21960784, 0.20392157, 0.21960784],\n",
       "        [0.22745098, 0.21176471, 0.23137255]],\n",
       "\n",
       "       [[0.16862745, 0.16078431, 0.17254902],\n",
       "        [0.16470588, 0.15294118, 0.16862745],\n",
       "        [0.16078431, 0.14901961, 0.16470588],\n",
       "        ...,\n",
       "        [0.21568627, 0.2       , 0.21568627],\n",
       "        [0.20784314, 0.19215686, 0.20784314],\n",
       "        [0.21568627, 0.20392157, 0.21568627]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.18039216, 0.17254902, 0.18431373],\n",
       "        [0.18431373, 0.17647059, 0.18823529],\n",
       "        [0.18431373, 0.17254902, 0.18431373],\n",
       "        ...,\n",
       "        [0.21960784, 0.20392157, 0.21568627],\n",
       "        [0.21176471, 0.19607843, 0.21176471],\n",
       "        [0.21176471, 0.19607843, 0.20784314]],\n",
       "\n",
       "       [[0.18039216, 0.16862745, 0.18431373],\n",
       "        [0.18039216, 0.16862745, 0.18039216],\n",
       "        [0.19607843, 0.18039216, 0.19607843],\n",
       "        ...,\n",
       "        [0.21176471, 0.19215686, 0.20784314],\n",
       "        [0.21960784, 0.20784314, 0.21960784],\n",
       "        [0.21568627, 0.20392157, 0.21176471]],\n",
       "\n",
       "       [[0.17254902, 0.16470588, 0.17647059],\n",
       "        [0.17647059, 0.16470588, 0.18039216],\n",
       "        [0.17647059, 0.16862745, 0.18039216],\n",
       "        ...,\n",
       "        [0.21960784, 0.20392157, 0.21568627],\n",
       "        [0.21568627, 0.2       , 0.21176471],\n",
       "        [0.22352941, 0.20784314, 0.22352941]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0121c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x29e7cded0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(patches[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1501dfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify_image_3d(image_data, patch_size):\n",
    "    \"\"\"\n",
    "    Function to patchify a 3D array of image data.\n",
    "\n",
    "    Args:\n",
    "    - image_data (np.array): Input image data of shape (W, H, D).\n",
    "    - patch_size (int): Size of each patch.\n",
    "\n",
    "    Returns:\n",
    "    - patches (np.array): Patchified image in the shape of (num_patches, patch_size, patch_size, D).\n",
    "    - num_patches (int): Number of patches.\n",
    "    \"\"\"\n",
    "    img_height, img_width, num_channels = image_data.shape\n",
    "\n",
    "    # Calculate number of patches in height and width\n",
    "    num_patches_h = img_height // patch_size\n",
    "    num_patches_w = img_width // patch_size\n",
    "\n",
    "    # Crop the image to fit an integer number of patches\n",
    "    image_data = image_data[:num_patches_h * patch_size, :num_patches_w * patch_size, :]\n",
    "\n",
    "    # Reshape image into patches\n",
    "    patches = np.reshape(image_data, (num_patches_h, patch_size, num_patches_w, patch_size, num_channels))\n",
    "    patches = np.transpose(patches, (0, 2, 1, 3, 4))\n",
    "    patches = np.reshape(patches, (-1, patch_size, patch_size, num_channels))\n",
    "\n",
    "    return patches, num_patches_h * num_patches_w\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a 3D array called 'image_data'\n",
    "image_data = np.random.rand(256, 256, 3)  # Example random image data\n",
    "patch_size = 32  # Choose appropriate patch size\n",
    "patches, num_patches = patchify_image_3d(image_data, patch_size)\n",
    "print(\"Number of patches:\", num_patches)\n",
    "print(\"Shape of patches:\", patches.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
